{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3639a352-86bf-4a41-9b4a-1d88304c4c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "\n",
    "import json\n",
    "\n",
    "with open(\"test.json\", \"r\") as json_file:\n",
    "    test_data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52da817-8d44-4fba-97bc-74dbc1a24360",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_input(entry):\n",
    "    instruction_text = (\n",
    "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
    "    )\n",
    "\n",
    "    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
    "\n",
    "    return instruction_text + input_text\n",
    "\n",
    "print(format_input(test_data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79331be-add6-44e5-8708-9ee40f2ff827",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_thesis = {\n",
    "    \"instruction\" : \"\"\"A Multi-Task Neural Network (MTNN) is a type of network in the field of machine learning\n",
    " where a single large neural network learns to solve multiple tasks simultaneously. In this paper,\n",
    " the performance of MTNNs is compared with that of corresponding Single-Task Neural Networks\n",
    " (STNN). The primary focus of this paper is on four autonomous driving perception tasks: semantic\n",
    " segmentation, lane marking, drivable area, and object detection. The Audi Autonomous Driving\n",
    " Dataset (A2D2) is used for the experiments. The dataset contains semantic segmentation labels\n",
    " for 57 classes (including labels for lane marking and drivable area) and 2D bounding boxes for\n",
    " 14 classes for 12,497 images. Here, we first implement and train STNNs, then MTNNs for all\n",
    " the combinations of our four primary tasks. The MTNN used here has a shared encoder and one\n",
    " decoder for each task. Introduced new loss weighing and training techniques for better optimization\n",
    " of MTNNs. Based on the experiment results, multi-task learning outperforms single-task learning\n",
    " both in speed and accuracy. MTNNs are up to 33% faster, with improvement in accuracy of\n",
    " semantic segmentation up to 1%, lane marking up to 2.5%, drivable area up to 1.5%, and object\n",
    " detection up to 12%. Further, transfer learning method is used to train STNNs with MTNN’s\n",
    " shared encoder weights. Results show improvement in STNNs accuracy up to 2%. Also, it shows\n",
    " that MTNN’s shared encoder has learned better generalized feature representations on the A2D2\n",
    " dataset. Finally, an MTNN with object detection as its primary task and semantic segmentation\n",
    " as an auxiliary task is trained on A2D2 dataset. The labels for auxiliary task were generated using\n",
    " DeepLabv3 architecture model pre-trained on CityScapes and BDD dataset. The results show that\n",
    " the object detection mAP score was improved by 6.94%.\"\"\",\n",
    "    \"input\" : \"What is MTNN?\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2370ba-e728-4ef7-9742-385fc985622f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from litgpt import LLM\n",
    "\n",
    "llm = LLM.load(\"meta_llama_32_finetuned_on_sqaud/finetune/lora/final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166591e5-27b8-49ec-bb19-df8b033be484",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm2 = LLM.load(\"tiny_llama_lora_fintune_on_squad_v1/final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96d36b6-6bee-45bf-917e-55bf88b0ba39",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm3 = LLM.load(\"tiny_llama_lora_fintune_on_squad_v1-v2-fu rther2-lr6e1/final\")\n",
    "llm4 = LLM.load(\"tiny_llama_lora_fintune_on_squad_v1-v2-further-lr6e6/final\")\n",
    "llm5 = LLM.load(\"tiny_llama_lora_finetune_on_squad_v1-v2/final\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef173e2e-580c-4ac5-a9c2-b4318dde58a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the training data\n",
    "with open(\"qa_dataset/train_squad_v1_v2_hotpot.json\", \"r\") as json_file:\n",
    "    train_data = json.load(json_file)\n",
    "\n",
    "# Initialize the tokenizer (use a general tokenizer, like LLama or GPT-2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")  # Replace with your model's tokenizer if different\n",
    "\n",
    "# Function to format input as in your provided code\n",
    "def format_input(entry):\n",
    "    instruction_text = f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
    "    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
    "    return instruction_text + input_text\n",
    "\n",
    "# Calculate the maximum token size\n",
    "max_token_count = 0\n",
    "max_entry = None  # To keep track of the longest entry\n",
    "tokens_morethan_2048 = 0\n",
    "\n",
    "examples_less_than_2048 = []\n",
    "for entry in tqdm(train_data):\n",
    "    formatted_text = format_input(entry)\n",
    "    tokenized = tokenizer(formatted_text, truncation=False, return_tensors=None)\n",
    "    token_count = len(tokenized[\"input_ids\"])\n",
    "    \n",
    "    if token_count > 1800:\n",
    "        tokens_morethan_2048 += 1\n",
    "        continue\n",
    "    examples_less_than_2048.append(entry)\n",
    "        #max_entry = entry\n",
    "print(f\"Examples more than 2048 tokens : {tokens_morethan_2048}\")\n",
    "#print(f\"Maximum token size: {max_token_count}\")\n",
    "#print(\"Longest entry:\")\n",
    "#print(format_input(max_entry))\n",
    "\n",
    "with open(\"train_v1_v2_hotpot_max_1800.json\", \"w\") as file2:\n",
    "    json.dump(examples_less_than_2048, file2, indent=4)\n",
    "\n",
    "\"\"\"\n",
    "for test_squad_v1_v2_hotpot.json\n",
    "Examples more than 2048 tokens : 112\n",
    "Maximum token size: 0\n",
    "Longest entry:\n",
    "\n",
    "for train_squad_v1_v2_hotpot.json\n",
    "Examples more than 2048 tokens : 2175\n",
    "Examples more than 2000 tokens : \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863ade4c-c003-4ce3-a8f8-223a2ecd4d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample = input_thesis\n",
    "test_sample[\"input\"] = \"How much MTNNs are better than STNNs in terms of inference speed?\"\n",
    "print(\"llm1\",llm.generate(format_input(test_sample), top_k=1))\n",
    "print(\"llm2\",llm2.generate(format_input(test_sample), top_k=1))\n",
    "print(\"llm3\", llm3.generate(format_input(test_sample), top_k=1))\n",
    "print(\"llm4\", llm3.generate(format_input(test_sample), top_k=1))\n",
    "print(\"llm5\", llm3.generate(format_input(test_sample), top_k=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d857547-d99b-422f-afd2-1401682c80f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the training data\n",
    "with open(\"qa_dataset/train_squad_v1_v2_hotpot.json\", \"r\") as json_file:\n",
    "    train_data = json.load(json_file)\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")  # Replace with your model's tokenizer if different\n",
    "max_token_length = 1400  # Define max token limit\n",
    "\n",
    "# Function to format input\n",
    "def format_input(entry):\n",
    "    instruction_text = f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
    "    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
    "    return instruction_text + input_text\n",
    "\n",
    "# Filter entries\n",
    "examples_less_than_max_tokens = []\n",
    "tokens_more_than_limit = 0\n",
    "\n",
    "for idx, entry in enumerate(tqdm(train_data, desc=\"Processing entries\")):\n",
    "    try:\n",
    "        formatted_text = format_input(entry)\n",
    "        tokenized = tokenizer(formatted_text, truncation=False, return_tensors=None)\n",
    "        token_count = len(tokenized[\"input_ids\"])\n",
    "\n",
    "        # Debug print for token count\n",
    "        if idx < 5:  # Print for the first few entries only\n",
    "            print(f\"Entry {idx} token count: {token_count}\")\n",
    "            print(f\"Formatted text: {formatted_text[:100]}...\")  # Print a snippet of the text\n",
    "\n",
    "        if token_count > max_token_length:\n",
    "            tokens_more_than_limit += 1\n",
    "            continue\n",
    "\n",
    "        examples_less_than_max_tokens.append(entry)\n",
    "    except KeyError as e:\n",
    "        print(f\"Missing key in entry {idx}: {e}\")\n",
    "\n",
    "# Summary\n",
    "print(f\"Total examples: {len(train_data)}\")\n",
    "print(f\"Examples exceeding {max_token_length} tokens: {tokens_more_than_limit}\")\n",
    "print(f\"Examples within token limit: {len(examples_less_than_max_tokens)}\")\n",
    "\n",
    "# Save filtered data\n",
    "output_file = \"train_v1_v2_hotpot_1400.json\"\n",
    "with open(output_file, \"w\") as file:\n",
    "    json.dump(examples_less_than_max_tokens, file, indent=4)\n",
    "print(f\"Filtered data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa39d785-5924-4ac3-9e8c-797c34858887",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cd1e4b-9b57-4594-9136-5fe85405c53d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
