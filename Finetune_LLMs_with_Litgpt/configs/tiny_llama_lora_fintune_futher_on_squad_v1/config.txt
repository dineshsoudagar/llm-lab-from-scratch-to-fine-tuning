(venv) PS C:\Others\LLMs\litgpt_scripts> litgpt finetune_lora tiny_llama_lora_fintune_on_squad_v1/final --data JSON --data.val_split_fraction 0.05 --data.json_path train.json --train.epochs 2 --train.log_interval 50 --precision bf16-true --train.micro_batch_size 2 --eval.interval 1000 --out_dir "tiny_llama_adapter_fintune_futher_on_squad_v1" --train.min_lr 0.000006
uvloop is not installed. Falling back to the default asyncio event loop. Please install uvloop for better performance using `pip install uvloop`.
uvloop is not installed. Falling back to the default asyncio event loop.
{'access_token': None,
 'checkpoint_dir': WindowsPath('tiny_llama_lora_fintune_on_squad_v1/final'),
 'data': JSON(json_path=WindowsPath('train.json'),
              mask_prompt=False,
              val_split_fraction=0.05,
              prompt_style=<litgpt.prompts.Alpaca object at 0x000001AD1B167150>,
              ignore_index=-100,
              seed=42,
              num_workers=4),
 'devices': 1,
 'eval': EvalArgs(interval=1000,
                  max_new_tokens=100,
                  max_iters=100,
                  initial_validation=False,
                  final_validation=True,
                  evaluate_example='first'),
 'logger_name': 'csv',
 'lora_alpha': 16,
 'lora_dropout': 0.05,
 'lora_head': False,
 'lora_key': False,
 'lora_mlp': False,
 'lora_projection': False,
 'lora_query': True,
 'lora_r': 8,
 'lora_value': True,
 'num_nodes': 1,
 'optimizer': 'AdamW',
 'out_dir': WindowsPath('tiny_llama_adapter_fintune_futher_on_squad_v1'),
 'precision': 'bf16-true',
 'quantize': None,
 'seed': 1337,
 'train': TrainArgs(save_interval=1000,
                    log_interval=50,
                    global_batch_size=16,
                    micro_batch_size=2,
                    lr_warmup_steps=100,
                    lr_warmup_fraction=None,
                    epochs=2,
                    max_tokens=None,
                    max_steps=None,
                    max_seq_length=None,
                    tie_embeddings=None,
                    max_norm=None,
                    min_lr=6e-06)}
Seed set to 1337
Number of trainable parameters: 1,126,400
Number of non-trainable parameters: 1,100,048,384
