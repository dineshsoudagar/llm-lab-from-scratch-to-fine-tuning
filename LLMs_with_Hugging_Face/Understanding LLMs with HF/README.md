# LLMs and Transformers with Hugging Face Library

This repository provides a collection of Jupyter notebooks designed to help users understand and effectively use the Hugging Face library. It covers key concepts and practical applications of large language models (LLMs) and transformers.

## Contents

- [Accessing Pre-Trained Models](Accessing_Pre_Trained_Models.ipynb)  
  Explore how to load and use pre-trained models from the Hugging Face library.  

- [Creating a Custom Tokenizer](Creating_A_Tokenizer.ipynb)  
  Learn how to build a tokenizer tailored to your specific use case.  

- [Document Question-Answering](Document_QA_Model.ipynb)  
  Implement a question-answering system for documents using pre-trained models.  

- [Full Fine-Tuning for Summarization](Full_Fine_Tuning_for_Summarization.ipynb)  
  Perform complete fine-tuning of transformer models for text summarization.  

- [Working with Datasets](Working_With_Datasets.ipynb)  
  Understand how to load, preprocess, and utilize datasets for training and evaluation.  

- [Quantizing Models](Quantizing_Models.ipynb)  
  Reduce model size and improve inference speed using quantization techniques.  

- [Reducing Precision](Reducing_Precision.ipynb)  
  Further optimize models by lowering precision without significant performance loss.  

- [Image-to-Text Applications](Image_to_Text.ipynb)  
  Utilize transformers for generating text from images.  

---

These notebooks provide hands-on examples and detailed explanations, making it easier to explore and understand the Hugging Face ecosystem.  
Feel free to use and modify them for your learning or projects!
